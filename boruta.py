# -*- coding: utf-8 -*-
"""BORUTA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WkJS9UUNbxNE2fw7YPPzAcp8cR_eZzEv
"""

import pandas as pd
import numpy as np
from tqdm.notebook import tqdm
import scipy as sp
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
from google.colab import drive

drive.mount("/content/drive") # establishing a connecttion between google drive

data = pd.read_csv("/content/drive/MyDrive/Dataset/healthcare_stroke_data.csv") # Read the datset from google drive
data.head(5)

"""## Data Pre-processing"""

#Converting to Numeral
data["gender"] = pd.factorize(data["gender"])[0]
data["ever_married"] = pd.factorize(data["ever_married"])[0]
data["work_type"] = pd.factorize(data["work_type"])[0]
data["Residence_type"] = pd.factorize(data["Residence_type"])[0]
data["smoking_status"] = pd.factorize(data["smoking_status"])[0]

#additional cleaning
data.dropna(inplace =True)
data.drop("id", axis =1, inplace = True)
data.head()

X = data.drop("stroke", axis = 1)
y = data["stroke"]

# Check for missing values in original features
print("Missing values in original features:")
print(X.isnull().sum())

"""## Creating shadow features"""

for col in X.columns:
    shuffled_values = np.random.permutation(X[col].values)
    X[f"shadow_{col}"] = shuffled_values

X.head(5)

# Check for missing values in original features
print("Missing values in original features:")
print(X.isnull().sum())

def get_important_features(X,y):
  rf =RandomForestClassifier(max_depth=20) # initilaize the random forest classifier

  rf.fit(X,y) # fit th model
  # Create dictionary of feature importances
  importances = {feature_name: f_importance for feature_name, f_importance in zip(X.columns, rf.feature_importances_)}
  # Isolate importances of Shadow features
  only_shadow_feat_importance = {key:value for key,value in importances.items() if "shadow" in key}
  # get importance level of most important shadow feature
  highest_shadow_feature = list(dict(sorted(only_shadow_feat_importance.items(), key=lambda item: item[1], reverse=True)).values())[0]
  # get original feature which fulfill boruta selection criteria
  selected_features = [key for key, value in importances.items() if value > highest_shadow_feature]
  return selected_features

"""## Iterate over"""

TRIALS = 50
feature_hits = {i:0 for i in data.columns}
for _ in tqdm(range(TRIALS)):
    imp_features = get_important_features(X, y)
    for key, _ in feature_hits.items():
      if key in imp_features: feature_hits[key] += 1
print(feature_hits)

"""## Binomial Distribution to select only the important features"""

# Calculate the probability mass function
pmf = [sp.stats.binom.pmf(x, TRIALS, .5) for x in range(TRIALS + 1)]
# trails_in_green_zone
def get_tail_items(pmf):
  total = 0
  for i, x in enumerate(pmf):
    total += x
    if total >= 0.05:
      break
  return i

# plot the binomial distribution
plt.plot([i for i in range(TRIALS + 1)], pmf,"-o")
plt.title(f"Binomial distribution for {TRIALS} trials")
plt.xlabel("No. of trials")
plt.ylabel("Probability")
plt.grid(True)

# select features from n number of trials
def choose_features(feature_hits, TRIALS, thresh):
    #define boundries
    green_zone_thresh = TRIALS - thresh
    blue_zone_upper = green_zone_thresh
    blue_zone_lower = thresh
    green_zone = [key for key, value in feature_hits.items() if    value >= green_zone_thresh]
    blue_zone = [key for key, value in feature_hits.items() if (value >= blue_zone_lower and value < blue_zone_upper)]
    return green_zone, blue_zone

thresh = get_tail_items(pmf)
green, blue = choose_features(feature_hits, TRIALS, thresh)
green,blue

print("Green Zone Features:", green)
print("Blue Zone Features:", blue)